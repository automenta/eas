#!/usr/bin/env python3
"""
aft_optimized.py - Optimized AFT Benchmark

Based on experimental findings:
1. Static AFT (1,024 params) beats complex dynamic approaches
2. Layer selection is critical - must sweep aggressively
3. Early stopping prevents overfitting
4. Some models respond better than others

This script implements an optimized AFT with:
- Aggressive layer sweep (every layer from 20% to 80% depth)
- Per-epoch evaluation with early stopping
- Multi-model support
- Results saved to JSON
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import numpy as np
import json
import os
import sys
import time
from datetime import datetime


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MODELS = {
    "pythia-410m": "EleutherAI/pythia-410m",
    "pythia-160m": "EleutherAI/pythia-160m", 
    "qwen-0.5b": "Qwen/Qwen1.5-0.5B",
    "qwen-1.8b": "Qwen/Qwen1.5-1.8B",
    "phi-1.5": "microsoft/phi-1_5",
}

DATASETS = ["hellaswag", "arc_challenge", "gsm8k"]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STATIC AFT (The approach that works)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class StaticAFT(nn.Module):
    """Simple static steering vector - the approach that actually works."""
    
    def __init__(self, model, layer_idx: int, hidden_dim: int):
        super().__init__()
        self.model = model
        self.layer_idx = layer_idx
        self.steering_vector = nn.Parameter(torch.zeros(1, 1, hidden_dim))
        self.hook_handle = None
        self._register_hook()
    
    def _get_layers(self):
        if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):
            return self.model.model.layers
        elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):
            return self.model.transformer.h
        elif hasattr(self.model, 'gpt_neox'):
            return self.model.gpt_neox.layers
        raise ValueError(f"Unknown architecture")
    
    def _register_hook(self):
        layers = self._get_layers()
        
        def hook_fn(module, input, output):
            hidden = output[0] if isinstance(output, tuple) else output
            vector = self.steering_vector.to(hidden.dtype).to(hidden.device)
            modified = hidden + vector
            return (modified,) + output[1:] if isinstance(output, tuple) else modified
        
        self.hook_handle = layers[self.layer_idx].register_forward_hook(hook_fn)
    
    def forward(self, input_ids, attention_mask=None):
        return self.model(input_ids, attention_mask=attention_mask)
    
    def generate(self, *args, **kwargs):
        return self.model.generate(*args, **kwargs)
    
    def cleanup(self):
        if self.hook_handle:
            self.hook_handle.remove()
    
    def get_vector_norm(self):
        return self.steering_vector.norm().item()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA LOADING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_data(dataset_name: str, max_samples: int = 200):
    """Load data in proper MC format."""
    samples = []
    
    if dataset_name == "hellaswag":
        ds = load_dataset("Rowan/hellaswag", split="validation")
        for i, ex in enumerate(ds):
            if i >= max_samples:
                break
            prompt = f"Complete:\n{ex['ctx']}\n"
            prompt += "\n".join([f"{chr(65+j)}. {opt}" for j, opt in enumerate(ex['endings'])])
            prompt += "\n\nAnswer:"
            samples.append({
                "prompt": prompt,
                "label": int(ex["label"]),
                "choices": ["A", "B", "C", "D"],
                "type": "mc"
            })
    
    elif dataset_name == "arc_challenge":
        ds = load_dataset("ai2_arc", "ARC-Challenge", split="validation")
        for i, ex in enumerate(ds):
            if i >= max_samples:
                break
            prompt = f"Question: {ex['question']}\n\nChoices:\n"
            choices = ex['choices']
            label_map = {label: idx for idx, label in enumerate(choices['label'])}
            for label, text in zip(choices['label'], choices['text']):
                prompt += f"{label}. {text}\n"
            prompt += "\nAnswer:"
            samples.append({
                "prompt": prompt,
                "label": label_map[ex['answerKey']],
                "choices": choices['label'],
                "type": "mc"
            })
    
    elif dataset_name == "gsm8k":
        ds = load_dataset("gsm8k", "main", split="test")
        for i, ex in enumerate(ds):
            if i >= max_samples:
                break
            prompt = f"Question: {ex['question']}\n\nAnswer:"
            answer = ex['answer'].split("####")[-1].strip()
            samples.append({
                "prompt": prompt,
                "answer": answer,
                "type": "gen"
            })
    
    # 50/50 split
    split = len(samples) // 2
    return samples[:split], samples[split:]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EVALUATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def evaluate(model, tokenizer, samples, device):
    """Evaluate accuracy."""
    correct = 0
    total = 0
    
    for s in samples:
        inputs = tokenizer(s["prompt"], return_tensors="pt", truncation=True, max_length=1024).to(device)
        
        if s["type"] == "mc":
            with torch.no_grad():
                logits = model(**inputs).logits[0, -1, :]
            
            probs = []
            for opt in s["choices"]:
                tid = tokenizer.encode(f" {opt}", add_special_tokens=False)
                if not tid:
                    tid = tokenizer.encode(opt, add_special_tokens=False)
                probs.append(logits[tid[0]].item() if tid else -float('inf'))
            
            if np.argmax(probs) == s["label"]:
                correct += 1
            total += 1
        
        elif s["type"] == "gen":
            with torch.no_grad():
                gen = model.generate(
                    **inputs, max_new_tokens=30,
                    pad_token_id=tokenizer.eos_token_id,
                    do_sample=False
                )
            output = tokenizer.decode(gen[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            if s["answer"] in output:
                correct += 1
            total += 1
    
    return correct / total * 100 if total > 0 else 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAINING WITH EARLY STOPPING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_with_early_stopping(steered, tokenizer, train_data, test_data, device, 
                               max_epochs=10, lr=1e-3, patience=3):
    """Train with early stopping based on test accuracy."""
    
    optimizer = torch.optim.Adam([steered.steering_vector], lr=lr)
    
    best_acc = 0
    best_vector = None
    epochs_without_improvement = 0
    
    for epoch in range(max_epochs):
        # Train
        epoch_loss = 0
        np.random.shuffle(train_data)
        
        for s in train_data:
            inputs = tokenizer(s["prompt"], return_tensors="pt", truncation=True, max_length=512).to(device)
            
            if s["type"] == "mc":
                target_token = f" {s['choices'][s['label']]}"
                target_ids = tokenizer.encode(target_token, add_special_tokens=False)
                if not target_ids:
                    continue
                
                outputs = steered(**inputs)
                logits = outputs.logits[0, -1, :]
                loss = F.cross_entropy(logits.float().unsqueeze(0), torch.tensor([target_ids[0]], device=device))
            else:
                # Generation task - train on answer
                answer_ids = tokenizer.encode(s["answer"], add_special_tokens=False)[:5]
                if not answer_ids:
                    continue
                
                outputs = steered(**inputs)
                logits = outputs.logits[0, -1, :]
                loss = F.cross_entropy(logits.float().unsqueeze(0), torch.tensor([answer_ids[0]], device=device))
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_([steered.steering_vector], 1.0)
            optimizer.step()
            optimizer.zero_grad()
            epoch_loss += loss.item()
        
        # Evaluate
        acc = evaluate(steered, tokenizer, test_data, device)
        
        if acc > best_acc:
            best_acc = acc
            best_vector = steered.steering_vector.data.clone()
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1
        
        # Early stopping
        if epochs_without_improvement >= patience:
            break
    
    # Restore best
    if best_vector is not None:
        steered.steering_vector.data = best_vector
    
    return best_acc


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LAYER SWEEP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def sweep_layers(model, tokenizer, train_data, test_data, hidden_dim, device, verbose=True):
    """Sweep layers to find the best one."""
    
    # Get layers
    if hasattr(model, 'gpt_neox'):
        layers = model.gpt_neox.layers
    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):
        layers = model.model.layers
    else:
        layers = model.transformer.h
    
    num_layers = len(layers)
    
    # Sweep from 20% to 80% depth
    start = int(num_layers * 0.2)
    end = int(num_layers * 0.8)
    
    best_layer = start
    best_acc = 0
    results = []
    
    if verbose:
        print(f"  Sweeping layers {start}-{end} (of {num_layers})...")
    
    for layer_idx in range(start, end + 1):
        steered = StaticAFT(model, layer_idx, hidden_dim)
        steered.steering_vector.data = steered.steering_vector.data.to(device).float()
        
        # Quick training (3 epochs) for sweep
        optimizer = torch.optim.Adam([steered.steering_vector], lr=1e-3)
        
        for _ in range(3):
            for s in train_data[:50]:  # Subset for speed
                inputs = tokenizer(s["prompt"], return_tensors="pt", truncation=True, max_length=512).to(device)
                
                if s["type"] == "mc":
                    target_token = f" {s['choices'][s['label']]}"
                    target_ids = tokenizer.encode(target_token, add_special_tokens=False)
                    if not target_ids:
                        continue
                    
                    outputs = steered(**inputs)
                    logits = outputs.logits[0, -1, :]
                    loss = F.cross_entropy(logits.float().unsqueeze(0), torch.tensor([target_ids[0]], device=device))
                    loss.backward()
                    optimizer.step()
                    optimizer.zero_grad()
        
        acc = evaluate(steered, tokenizer, test_data[:50], device)
        results.append({"layer": layer_idx, "acc": acc})
        
        if acc > best_acc:
            best_acc = acc
            best_layer = layer_idx
        
        steered.cleanup()
        
        if verbose:
            print(f"    Layer {layer_idx}: {acc:.1f}%")
    
    if verbose:
        print(f"  Best layer: {best_layer} ({best_acc:.1f}%)")
    
    return best_layer, results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN EXPERIMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_experiment(model_name: str, dataset_name: str, verbose=True):
    """Run full experiment with layer sweep and early stopping."""
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_path = MODELS.get(model_name, model_name)
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª {model_name} + {dataset_name}")
        print(f"{'='*60}")
    
    # Load model
    if verbose:
        print("  Loading model...")
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True
    )
    for p in model.parameters():
        p.requires_grad = False
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    hidden_dim = model.config.hidden_size
    
    # Load data
    if verbose:
        print("  Loading data...")
    train_data, test_data = load_data(dataset_name, 200)
    
    # Baseline
    if verbose:
        print("  Baseline evaluation...")
    baseline_acc = evaluate(model, tokenizer, test_data, device)
    if verbose:
        print(f"  Baseline: {baseline_acc:.1f}%")
    
    # Layer sweep
    if verbose:
        print("\n  Layer sweep...")
    best_layer, sweep_results = sweep_layers(model, tokenizer, train_data, test_data, hidden_dim, device, verbose)
    
    # Full training on best layer
    if verbose:
        print(f"\n  Full training on layer {best_layer}...")
    
    steered = StaticAFT(model, best_layer, hidden_dim)
    steered.steering_vector.data = steered.steering_vector.data.to(device).float()
    
    final_acc = train_with_early_stopping(
        steered, tokenizer, train_data, test_data, device,
        max_epochs=10, lr=1e-3, patience=3
    )
    
    improvement = final_acc - baseline_acc
    
    if verbose:
        emoji = "âœ…" if improvement > 0 else "âŒ" if improvement < 0 else "â¸ï¸"
        print(f"\n  {'='*50}")
        print(f"  ğŸ“‹ RESULT")
        print(f"  {'='*50}")
        print(f"  Baseline:    {baseline_acc:.1f}%")
        print(f"  AFT:         {final_acc:.1f}%")
        print(f"  {emoji} Improvement: {improvement:+.1f}%")
        print(f"  Best Layer:  {best_layer}")
        print(f"  Vector Norm: {steered.get_vector_norm():.4f}")
        print(f"  {'='*50}")
    
    steered.cleanup()
    
    # Save result
    result = {
        "model": model_name,
        "dataset": dataset_name,
        "baseline_acc": baseline_acc,
        "final_acc": final_acc,
        "improvement": improvement,
        "best_layer": best_layer,
        "sweep_results": sweep_results,
        "timestamp": datetime.now().isoformat()
    }
    
    os.makedirs("results/aft_optimized", exist_ok=True)
    filename = f"results/aft_optimized/{model_name}_{dataset_name}.json"
    with open(filename, "w") as f:
        json.dump(result, f, indent=2)
    
    return result


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Optimized AFT Benchmark")
    parser.add_argument("--model", default="pythia-410m", choices=list(MODELS.keys()))
    parser.add_argument("--dataset", default="hellaswag", choices=DATASETS)
    parser.add_argument("--all", action="store_true", help="Run all models x datasets")
    
    args = parser.parse_args()
    
    print(f"\n{'='*60}")
    print("ğŸš€ OPTIMIZED AFT BENCHMARK")
    print(f"{'='*60}")
    
    if args.all:
        all_results = []
        for model in MODELS.keys():
            for dataset in DATASETS:
                try:
                    result = run_experiment(model, dataset)
                    all_results.append(result)
                except Exception as e:
                    print(f"âŒ Error: {model}/{dataset}: {e}")
        
        # Summary
        print(f"\n{'='*60}")
        print("ğŸ“Š FINAL SUMMARY")
        print(f"{'='*60}")
        
        positive = [r for r in all_results if r["improvement"] > 0]
        negative = [r for r in all_results if r["improvement"] < 0]
        neutral = [r for r in all_results if r["improvement"] == 0]
        
        print(f"âœ… Positive: {len(positive)}/{len(all_results)} ({len(positive)/len(all_results)*100:.0f}%)")
        print(f"âŒ Negative: {len(negative)}/{len(all_results)}")
        print(f"â¸ï¸ Neutral:  {len(neutral)}/{len(all_results)}")
        
        if all_results:
            avg = sum(r["improvement"] for r in all_results) / len(all_results)
            print(f"ğŸ“ˆ Average:  {avg:+.1f}%")
        
        if positive:
            best = max(positive, key=lambda x: x["improvement"])
            print(f"ğŸ† Best: +{best['improvement']:.1f}% ({best['model']}/{best['dataset']})")
        
        print(f"{'='*60}\n")
    else:
        run_experiment(args.model, args.dataset)


if __name__ == "__main__":
    main()
