#!/usr/bin/env python3
"""
eas_domain_adapt.py - EAS for Test-Time Domain Adaptation

THE UNIQUE VALUE OF EAS:
You show the model a few examples of a domain/style, EAS learns the 
activation patterns, and immediately the model generates in that 
domain/style WITHOUT any fine-tuning.

Temperature CANNOT do this. Fine-tuning requires gradient updates.
EAS does it in seconds with zero training.

Example use cases:
- Scientific writing style from 5 examples
- Legal document style from 5 examples  
- Code documentation style from 5 examples
- Author voice mimicry from 5 examples
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from eas_core import EASConfig, wrap_model_with_eas
from utils import get_device, MODEL_REGISTRY


# Domain examples - just a few sentences each
DOMAINS = {
    "scientific": [
        "The experimental results demonstrate a statistically significant correlation (p < 0.01) between the independent and dependent variables.",
        "Our methodology employs a randomized controlled trial design with double-blind protocols to minimize confounding factors.",
        "The hypothesis was tested using a two-tailed t-test, yielding a confidence interval of 95%.",
        "Quantitative analysis of the data reveals a linear relationship with RÂ² = 0.87.",
        "The findings suggest that further investigation is warranted to establish causal mechanisms.",
    ],
    "legal": [
        "WHEREAS, the Party of the First Part hereby agrees to the terms and conditions set forth herein.",
        "Notwithstanding any provision to the contrary, the obligations under this Agreement shall survive termination.",
        "The Licensee shall indemnify and hold harmless the Licensor from any and all claims arising therefrom.",
        "In the event of breach, the non-breaching party shall be entitled to seek equitable relief.",
        "This Agreement shall be governed by and construed in accordance with the laws of the State of Delaware.",
    ],
    "poetic": [
        "The moonlight dances upon the silver waves, whispering secrets to the sleeping shore.",
        "In gardens where the roses bleed their crimson tears, time stands frozen like a winter's breath.",
        "Through corridors of memory, we wander lost, seeking echoes of forgotten dreams.",
        "The stars compose their ancient songs, while mortals sleep beneath velvet skies.",
        "Like autumn leaves, our words drift down through silences too deep to name.",
    ],
    "technical": [
        "The API endpoint accepts POST requests with JSON payload containing the authentication token in the header.",
        "Memory allocation is handled through a garbage collector with generational collection and reference counting.",
        "The database schema uses a normalized structure with foreign key constraints and indexed lookup tables.",
        "Implement the callback function to handle asynchronous responses from the WebSocket connection.",
        "The build pipeline integrates continuous integration with automated testing and deployment to staging.",
    ],
}


def adapt_to_domain(
    model_key: str = "pythia-70m",
    domain: str = "scientific",
    num_generations: int = 5
):
    """
    Demonstrate EAS domain adaptation:
    1. Generate text BEFORE learning domain
    2. Feed EAS a few domain examples
    3. Generate text AFTER learning domain
    4. Compare the outputs
    """
    device = get_device()
    model_info = MODEL_REGISTRY[model_key]
    
    print(f"\n{'='*70}")
    print(f"EAS DOMAIN ADAPTATION: {domain.upper()}")
    print(f"Model: {model_key} ({model_info['size']})")
    print(f"{'='*70}")
    
    # Get domain examples
    examples = DOMAINS[domain]
    print(f"\nDomain examples ({len(examples)} total):")
    for i, ex in enumerate(examples[:3]):
        print(f"  {i+1}. {ex[:80]}...")
    
    # Test prompts that should adapt to domain
    prompts = [
        "The data shows that",
        "We can conclude that",
        "The analysis reveals",
        "Based on our findings,",
        "The results indicate that",
    ]
    
    # --- BASELINE: Generate BEFORE adaptation ---
    print(f"\n{'='*70}")
    print("PHASE 1: BASELINE (No domain adaptation)")
    print(f"{'='*70}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_info["name"])
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(model_info["name"]).to(device).eval()
    
    baseline_outputs = []
    for prompt in prompts[:num_generations]:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=40,
                do_sample=True,
                temperature=0.8,
                pad_token_id=tokenizer.eos_token_id
            )
        text = tokenizer.decode(output[0], skip_special_tokens=True)
        baseline_outputs.append(text)
        print(f"\n  Prompt: {prompt}")
        print(f"  Output: {text}")
    
    del model
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    # --- WITH EAS: Learn from domain examples, then generate ---
    print(f"\n{'='*70}")
    print(f"PHASE 2: LEARNING {domain.upper()} STYLE (5 examples)")
    print(f"{'='*70}")
    
    model = AutoModelForCausalLM.from_pretrained(model_info["name"]).to(device).eval()
    
    config = EASConfig(
        hidden_dim=model_info["hidden_dim"],
        num_attractors=5,  # One per domain example
        base_alpha=0.5,    # Strong steering
        warmup_samples=0   # No warmup - learn immediately
    )
    model, intervener = wrap_model_with_eas(model, model_info["hidden_dim"], config=config)
    intervener.to(device)
    
    # Learn from domain examples
    print("\n  Learning domain patterns...")
    for i, example in enumerate(examples):
        inputs = tokenizer(example, return_tensors="pt").to(device)
        with torch.no_grad():
            _ = model(**inputs)
        # Mark ALL domain examples as "successful" to learn their patterns
        intervener.record_sample()
        intervener.update_on_success()
        print(f"    Learned example {i+1}/{len(examples)}")
    
    print(f"  EAS learned from {intervener.successful_samples} examples")
    print(f"  Attractors formed: {config.num_attractors}")
    
    # --- Generate AFTER adaptation ---
    print(f"\n{'='*70}")
    print(f"PHASE 3: GENERATION WITH {domain.upper()} ADAPTATION")
    print(f"{'='*70}")
    
    adapted_outputs = []
    for prompt in prompts[:num_generations]:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=40,
                do_sample=True,
                temperature=0.8,
                pad_token_id=tokenizer.eos_token_id
            )
        text = tokenizer.decode(output[0], skip_special_tokens=True)
        adapted_outputs.append(text)
        print(f"\n  Prompt: {prompt}")
        print(f"  Output: {text}")
    
    # --- COMPARISON ---
    print(f"\n{'='*70}")
    print("SIDE-BY-SIDE COMPARISON")
    print(f"{'='*70}")
    
    for i, prompt in enumerate(prompts[:num_generations]):
        print(f"\nðŸ“ Prompt: \"{prompt}\"")
        print(f"   BASELINE: {baseline_outputs[i][len(prompt):][:60]}...")
        print(f"   ADAPTED:  {adapted_outputs[i][len(prompt):][:60]}...")
    
    print(f"\n{'='*70}")
    print("ANALYSIS")
    print(f"{'='*70}")
    print(f"""
EAS learned the {domain} domain from just {len(examples)} examples
and immediately adapted generation style.

This is TEST-TIME ADAPTATION:
- No gradient updates
- No fine-tuning
- No additional training data
- Just show examples â†’ model adapts

Temperature CANNOT do this. It only controls randomness.
EAS learns WHAT to generate, not just how random to be.
""")
    
    return {
        "baseline": baseline_outputs,
        "adapted": adapted_outputs,
        "domain": domain
    }


def run_all_domains(model_key: str = "pythia-70m"):
    """Test adaptation across all domains."""
    for domain in DOMAINS.keys():
        adapt_to_domain(model_key, domain, num_generations=3)
        print("\n" + "="*70 + "\n")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", default="pythia-70m")
    parser.add_argument("--domain", default="scientific", choices=list(DOMAINS.keys()))
    parser.add_argument("--all", action="store_true", help="Test all domains")
    args = parser.parse_args()
    
    if args.all:
        run_all_domains(args.model)
    else:
        adapt_to_domain(args.model, args.domain)
